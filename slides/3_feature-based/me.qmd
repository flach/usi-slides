---
title: "Marginal Effect (ME)<br>/Marginal Plots, M-Plots or<br>Local Dependence Profiles/"
subtitle: "(Feature Influence)"
---

```{python}
#| echo: false
#| output: false

# Handle imports and setup
import scipy
import sklearn.datasets
import sklearn.inspection
import sklearn.tree
import sklearn.svm
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

plt.style.use('seaborn')
```

```{python}
#| echo: false
#| output: false

def update_colours(dark=False):
    solarized_dark = '#e4dbbd'
    solarized_light = '#002b36'
    colour = solarized_dark if dark else solarized_light
    mpl.rcParams.update({
        'text.color' : colour,
        'axes.labelcolor' : colour,
        'xtick.color': colour,
        'ytick.color': colour})

update_colours()
line_colour = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]
other_colour = plt.rcParams['axes.prop_cycle'].by_key()['color'][1]
third_colour = plt.rcParams['axes.prop_cycle'].by_key()['color'][2]
```

```{python}
#| echo: false
#| output: false

# Strict Marginal Effect (ME)
def me_strict(data, pred_fn, feature, class_, crisp=False):
    fu, _ = np.unique(data[:, feature], return_counts=True)

    fu_mean, fu_std = [], []
    for feature_value in fu:
        instance_idx = np.where(data[:, feature] == feature_value)[0]
        instances = data[instance_idx, :]

        if crisp:
            me_mean = [0, 0, 0]
            me_std = np.nan
            elems, counts = np.unique(
                pred_fn(instances), return_counts=True)
            for i, j in zip(elems, counts):
                me_mean[i] += j
        else:
            me = pred_fn(instances)[:, class_]
            me_mean = me.mean()
            me_std = me.std()

        fu_mean.append(me_mean)
        fu_std.append(me_std)

    return fu, np.array(fu_mean), np.array(fu_std)

def me_strict_2d(data, pred_fn, features, class_):
    fu0, _ = np.unique(data[:, features[0]], return_counts=True)
    fu1, _ = np.unique(data[:, features[1]], return_counts=True)

    fu_mean, fu_std = [], []
    for feature_value0 in fu0:
        fu_mean_, fu_std_ = [], []
        for feature_value1 in fu1:
            instance_idx0 = np.where(data[:, features[0]] == feature_value0)[0]
            instance_idx1 = np.where(data[:, features[1]] == feature_value1)[0]
            instance_idx = list(set(instance_idx0).intersection(instance_idx1))

            instances = data[instance_idx, :]

            if instances.shape[0]:
                me = pred_fn(instances)[:, class_]
                me_mean = me.mean()
                me_std = me.std()
            else:
                me_mean = np.nan
                me_std = np.nan

            fu_mean_.append(me_mean)
            fu_std_.append(me_std)

        fu_mean.append(fu_mean_)
        fu_std.append(fu_std_)

    return (fu0, fu1), np.array(fu_mean), np.array(fu_std)

# Relaxed Marginal Effect (ME)
def me_relaxed(data, pred_fn, feature, class_, bins=10):
    edges = np.histogram_bin_edges(data[:, feature], bins=bins)
    edges_ = np.copy(edges)
    edges[0] = edges[0] - 0.1
    edges[bins] = edges[bins] + 0.1

    fu = (edges + (np.roll(edges, shift=-1) - edges) / 2)[:-1]

    bin_ids = np.digitize(data[:, feature], edges)
    assert 0 not in bin_ids
    assert bins + 1 not in bin_ids

    fu_mean, fu_std, fu_count = [], [], []
    for i in range(1, bins + 1):
        instance_idx = (bin_ids == i)
        instances = data[instance_idx, :]

        fu_count.append(instances.shape[0])

        if instances.shape[0]:
            me = pred_fn(instances)[:, class_]
            me_mean = me.mean()
            me_std = me.std()
        else:
            me_mean = np.nan
            me_std = np.nan

        fu_mean.append(me_mean)
        fu_std.append(me_std)

    return edges_, fu, np.array(fu_mean), np.array(fu_std), fu_count
```

```{python}
#| echo: false
#| output: false

# Get data and model ready
# Iris -- numerical
data_dict = sklearn.datasets.load_iris()
feature_names, target_names = data_dict['feature_names'], data_dict['target_names']

X_num, y_num = data_dict['data'], data_dict['target']
clf_num = sklearn.svm.SVC(probability=True)
clf_num.fit(X_num, y_num)

# Iris -- categorical
X_cat, y_cat = np.array(X_num), y_num
_map = X_num[:, 3] < 0.8
X_cat[_map, 3] = 0
_map = np.logical_and((X_num[:, 3] >= 0.8), (X_num[:, 3] < 1.35))
X_cat[_map, 3] = 1
_map = X_num[:, 3] >= 1.35
X_cat[_map, 3] = 2
cat_map = {0: 'low (x<0.8)', 1: 'medium (0.8≤x<1.35)', 2: 'high (1.35≤x)'}
clf_cat = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=10)
clf_cat.fit(X_cat, y_cat)
```

# Method Overview

## Explanation Synopsis

<br>

> ME captures the **average response of a predictive model** across
> a **collection of instances** (taken from a designated data set)
> for a **specific value of a selected feature**
> (found in the aforementioned data set) [@apley2020visualizing].
> This measure can be **relaxed** by including **similar feature values**
> determined by a fixed interval around the selected value.

<br>

> It communicates **global** (with respect to the *entire* explained model)
> **feature influence**.

## Rationale

<br>

> ME improves upon
> [{{< fa person-chalkboard >}} Partial Dependence (PD)](pd.html)
> [@friedman2001greedy] by ensuring that the influence estimates are based on
> **realistic instances** (thus respecting *feature correlation*),
> making the explanatory insights more truthful.

<br>

::: {.callout-caution}
## Method's Name

Note that even though the *Marginal Effect* name suggests that these
explanations are based on the *marginal distribution* of the selected feature,
they are actually derived from its *conditional distribution*.
:::

## Toy Example -- Strict ME -- Numerical Feature

```{python}
#| echo: false
#| output: false

# Generate M
class_id = 0
feature_id = 2
me_x, me_y, me_y_std = me_strict(
    X_num, clf_num.predict_proba, feature_id, class_id)
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Strict ME for a numerical feature
#| fig-width: 55%

plt.style.use('default')
with plt.xkcd():
  fig, ax = plt.subplots(figsize=(8, 4))
  fig.patch.set_alpha(0)

  plt.plot(me_x, me_y)
  plt.vlines(me_x, 0, 1, alpha=.5, zorder=0,
             lw=1.0, ls=(0, (1, 1)), color=other_colour)

  plt.xlabel(f'{feature_names[feature_id]} value'.upper())

  plt.ylabel(f'{target_names[class_id]} probability'.upper())
  ax.yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
  # ax.set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

  plt.title(f'Strict ME for the Iris data set'.upper())

plt.show()
plt.style.use('seaborn')
update_colours()
```

## Toy Example -- Relaxed ME -- Numerical Feature

```{python}
#| echo: false
#| output: false

# Generate M
class_id = 0
feature_id = 2
me_edges, me_x, me_y, me_y_std, _ = me_relaxed(
    X_num, clf_num.predict_proba, feature_id, class_id)
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Relaxed ME for a numerical feature
#| fig-width: 55%

plt.style.use('default')
with plt.xkcd():
  fig, ax = plt.subplots(figsize=(8, 4))
  fig.patch.set_alpha(0)

  plt.plot(me_x, me_y, zorder=10)
  mask = np.isfinite(me_y)
  plt.plot(me_x[mask], me_y[mask], ls='--', c=line_colour, zorder=5)
  plt.vlines(me_x, 0, 1, alpha=1.0, zorder=0,
             lw=1.0, ls=(0, (3, 5, 1, 5)), color=other_colour)

  where_ = [4, 5]
  plt.vlines(me_edges[where_], 0, 1, alpha=.5, zorder=0,
             lw=.5, ls='-', color=third_colour)
  plt.fill_betweenx([0, 1], me_edges[where_[0]], me_edges[where_[1]],
                    alpha=.25, zorder=0, color=third_colour)

  plt.xlabel(f'{feature_names[feature_id]} value'.upper())

  plt.ylabel(f'{target_names[class_id]} probability'.upper())
  ax.yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
  # ax.set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

  plt.title(f'Relaxed ME for the Iris data set'.upper())

plt.show()
plt.style.use('seaborn')
update_colours()
```

## Toy Example -- Strict ME -- Categorical Feature

```{python}
#| echo: false
#| output: false

# Generate M
class_id = 0
feature_id = 3
me_x, me_y, me_y_std = me_strict(
    X_cat, clf_cat.predict_proba, feature_id, class_id)
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Strict ME for a categorical feature
#| fig-width: 55%

plt.style.use('default')
with plt.xkcd():
  fig, ax = plt.subplots(figsize=(8, 4))
  fig.patch.set_alpha(0)

  plt.scatter(me_x, me_y, zorder=5, marker='X', s=100)

  plt.xlabel(f'{feature_names[feature_id][:-5]} category'.upper())
  plt.xticks([0, 1, 2])
  ax.xaxis.set_major_formatter(lambda f, pos: f'{cat_map[f]}')

  plt.ylabel(f'{target_names[class_id]} probability'.upper())
  ax.yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
  # ax.set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

  plt.title(f'Strict ME for the Iris data set'.upper())

plt.show()
plt.style.use('seaborn')
update_colours()
```

## Method Properties 

<br>

| *Property*           | **Marginal Effect**                                   |
|----------------------|-------------------------------------------------------|
| *relation*           | post-hoc                                              |
| *compatibility*      | model-agnostic                                        |
| *modelling*          | regression, crisp and probabilistic classification    |
| *scope*              | global (per data set; generalises to cohort)          |
| *target*             | model (set of predictions)                            |

## Method Properties {{< meta subs.ctd >}}

<br>

| *Property*           | **Marginal Effect**                                   |
|----------------------|-------------------------------------------------------|
| *data*               | tabular                                               |
| *features*           | numerical and categorical                             |
| *explanation*        | feature influence (visualisation)                     |
| *caveats*            | feature correlation, heterogeneous model response     |

<!-- #%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#% --->

# (Algorithmic) Building Blocks

## Computing ME

<br>

::: {.callout-note}
## Input

1. Select a **feature to explain**
2. Select the **explanation target**

    * crisp classifiers &rarr; one(-vs.-the-rest) or all classes
    * probabilistic classifiers &rarr; (probabilities of) one class
    * regressors &rarr; numerical values

3. Select a **collection of instances** to generate the explanation
:::

## Computing ME {{< meta subs.ctd >}}

<br>

::: {.callout-caution}
## Parameters

1. If using the **relaxed** ME, define **binning** of the explained feature

    * numerical attributes &rarr; specify (fixed-width or quantile) binning or
      values of interest with a allowed variation
    * categorical attributes &rarr; the full set, a subset or grouping
      of possible values
:::

## Computing ME {{< meta subs.ctd >}}

<br>

::: {.callout-tip}
## Procedure

1. If unavailable, collect predictions of the designated data set
2. For each instance in this set

    * for **exact** ME, assign it to a collection based on its value of
      the explained feature (possibly multiple instance per value)
    * for **relaxed** ME, assign it to a bin that spans the range to which
      the value of its explained feature belongs

3. Generate and plot *Marginal Effect*

    * for **crisp classifiers** **count** the number of each unique prediction
      across all the instances collected for every value (**exact**) or
      bin (**relaxed**) of the explained feature;
      visualise ME either as a *count* or *proportion* using separate line
      for each class or using a stacked bar chart
    * for **probabilistic classifiers** (per class) and **regressors**
      **average** the response of the model across all the instances collected
      for each value (**exact**) or bin (**relaxed**) of the explained feature;
      visualise ME as a line

{{< fa star >}}&nbsp;&nbsp;&nbsp; Since the values of the explained feature may
**not be uniformly distributed** in the underlying data set,
a **rug plot** showing the distribution of its feature values for **strict** ME
or a **histogram** representing the number of instances per bin in
**relaxed** ME can help in interpreting the explanation.
:::

<!-- #%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#% --->

# Theoretical Underpinning

## Formulation &nbsp;&nbsp;&nbsp;{{< fa square-root-alt >}}

$$
X_{\mathit{ME}} \subseteq \mathcal{X}
$$

$$
V_i = \{ x_i : x \in X_{\mathit{ME}} \}
$$

$$
\mathit{ME}_i =
\mathbb{E}_{X_{\setminus i} | X_{i}} \left[ f \left( X_{\setminus i} , X_{i} \right) | X_{i}=v_i \right] =
\int_{X_{\setminus i}} f \left( X_{\setminus i} , x_i \right) \; d \mathbb{P} ( X_{\setminus i} | X_i = v_i )
\;\; \forall \; v_i \in V_i
$$

<br>

$$
\mathit{ME}_i =
\mathbb{E}_{X_{\setminus i} | X_{i}} \left[ f \left( X_{\setminus i} , X_{i} \right) | X_{i}=V_i \right] =
\int_{X_{\setminus i}} f \left( X_{\setminus i} , x_i \right) \; d \mathbb{P} ( X_{\setminus i} | X_i = V_i )
$$

## Formulation &nbsp;&nbsp;&nbsp;{{< fa square-root-alt >}} {{< meta subs.ctd >}}

<br>

Based on the ICE notation [@goldstein2015peeking]

<br>

$$
\left\{ \left( x_{S}^{(i)} , x_{C}^{(i)} \right) \right\}_{i=1}^N
$$

<br>

$$
\hat{f}_S =
\mathbb{E}_{X_{C} | X_S} \left[ \hat{f} \left( X_{S} , X_{C} \right) | X_S = x_S \right] =
\int_{X_C} \hat{f} \left( x_{S} , X_{C} \right) \; d \mathbb{P} ( X_{C} | X_S = x_S )
$$

::: {.notes}
- $x_S$ is **fixed** -- the explained feature
- $x_C$ are the **given** feature values
- $X_C$ and $X_S$ are the **random variables**

- **Conditioning** the predictions on the distribution of the **explained**
  feature(s) yields (**average**) dependence between the explained feature(s)
  (*including any interactions*) and predictions.
:::

## Approximation &nbsp;&nbsp;&nbsp;{{< fa desktop >}}

<br>

$$
\mathit{ME}_i \approx
\frac{1}{\sum_{x \in X_{\mathit{ME}}} \mathbb{1} (x_i = v_i)} \sum_{x \in X_{\mathit{ME}}}
f \left( x | x_i=v_i \right)
$$

<!-- #%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#% --->

# Variants

## Relaxed ME

<br>

> Measures ME for a range of values $v_i \pm \delta$ around a selected value
> $v_i$, instead of doing so precisely at that point.


$$
\mathit{ME}_i^{\pm\delta} =
\mathbb{E}_{X_{\setminus i} | X_{i}} \left[ f \left( X_{\setminus i} , X_{i} \right) | X_{i}=v_i \pm \delta \right] =
\int_{X_{\setminus i}} f \left( X_{\setminus i} , x_i \right) \; d \mathbb{P} ( X_{\setminus i} | X_i = v_i \pm \delta )
\;\; \forall \; v_i \in V_i
$$

<p style="text-align: center;">or</p>

$$
\hat{f}_S^{\pm\delta} =
\mathbb{E}_{X_{C} | X_S} \left[ \hat{f} \left( X_{S} , X_{C} \right) | X_S = x_S \pm \delta \right] =
\int_{X_C} \hat{f} \left( x_{S} , X_{C} \right) \; d \mathbb{P} ( X_{C} | X_S = x_S \pm \delta )
$$

::: {.notes}
- We can get a more robust measurement of feature influence by relaxing
  the value for which ME is computed.
:::

<!-- #%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#% --->

# Examples

## Strict ME

```{python}
#| echo: false
#| output: false

# Generate M
class_id = 0
feature_id = 2
me_x, me_y, me_y_std = me_strict(
    X_num, clf_num.predict_proba, feature_id, class_id)
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Strict ME
#| fig-width: 55%

fig, ax = plt.subplots(2, 1, figsize=(8, 4.5), height_ratios=[4, .5], sharex=True)
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=0, hspace=.05)

# ME
plt.sca(ax[0])

plt.title(f'Strict ME for the Iris data set')

plt.plot(me_x, me_y)
plt.vlines(me_x, 0, 1, alpha=.5, zorder=0,
            lw=1.0, ls=(0, (1, 1)), color=other_colour)

plt.ylabel(f'{target_names[class_id]} probability')
plt.gca().yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
# plt.gca().set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

# rug plot
plt.sca(ax[1])

plt.xlabel(f'{feature_names[feature_id]} value')
plt.yticks([])

kde1 = scipy.stats.gaussian_kde(X_num[:, feature_id])
rug_x = np.linspace(
    X_num[:, feature_id].min(),
    X_num[:, feature_id].max(),
    num=200)
plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

plt.scatter(
    X_num[:, feature_id], [0]*len(X_num[:, feature_id]),
    c='k', marker='|', s=100, alpha=.33, zorder=10)

plt.show()
```

## Strict ME with Standard Deviation

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Strict ME with Standard Deviation
#| fig-width: 55%

fig, ax = plt.subplots(2, 1, figsize=(8, 4.5), height_ratios=[4, .5], sharex=True)
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=0, hspace=.05)

# ME
plt.sca(ax[0])

plt.title(f'Strict ME for the Iris data set')

plt.plot(me_x, me_y)
plt.fill_between(
    me_x,
    (me_y - me_y_std),
    (me_y + me_y_std),
    color=line_colour,
    alpha=.33)
plt.vlines(me_x, 0, 1, alpha=.5, zorder=0,
            lw=1.0, ls=(0, (1, 1)), color=other_colour)

plt.ylabel(f'{target_names[class_id]} probability')
plt.gca().yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
# plt.gca().set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

# rug plot
plt.sca(ax[1])

plt.xlabel(f'{feature_names[feature_id]} value')
plt.yticks([])

kde1 = scipy.stats.gaussian_kde(X_num[:, feature_id])
rug_x = np.linspace(
    X_num[:, feature_id].min(),
    X_num[:, feature_id].max(),
    num=200)
plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

plt.scatter(
    X_num[:, feature_id], [0]*len(X_num[:, feature_id]),
    c='k', marker='|', s=100, alpha=.33, zorder=10)

plt.show()
```

## Centred Strict ME (with Standard Deviation)

```{python}
#| echo: false
#| output: false

# Generate M
cme_num_y = me_y - me_y[0]
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Centred Strict ME with Standard Deviation
#| fig-width: 55%

fig, ax = plt.subplots(2, 1, figsize=(8, 4.5), height_ratios=[4, .5], sharex=True)
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=0, hspace=.05)

# ME
plt.sca(ax[0])

plt.title(f'Strict ME for the Iris data set')

plt.plot(me_x, cme_num_y)
plt.fill_between(
    me_x,
    (cme_num_y - me_y_std),
    (cme_num_y + me_y_std),
    color=line_colour,
    alpha=.33)
plt.vlines(me_x, -1, 0, alpha=.5, zorder=0,
            lw=1.0, ls=(0, (1, 1)), color=other_colour)

plt.ylabel(f'{target_names[class_id]} probability')
plt.gca().yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
# plt.gca().set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

# rug plot
plt.sca(ax[1])

plt.xlabel(f'{feature_names[feature_id]} value')
plt.yticks([])

kde1 = scipy.stats.gaussian_kde(X_num[:, feature_id])
rug_x = np.linspace(
    X_num[:, feature_id].min(),
    X_num[:, feature_id].max(),
    num=200)
plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

plt.scatter(
    X_num[:, feature_id], [0]*len(X_num[:, feature_id]),
    c='k', marker='|', s=100, alpha=.33, zorder=10)

plt.show()
```

## Strict ME for Two (Numerical) Features

```{python}
#| echo: false
#| output: false

# Generate M
class_id = 0
features_id = [0, 1]
me_x_2d, me_y_2d, me_y_2d_std = me_strict_2d(
    X_num, clf_num.predict_proba, features_id, class_id)
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Two-dimensional Strict ME for the Iris data set
#| fig-width: 55%

fig, ax = plt.subplots(  # 12 x 6
    2, 3, figsize=(12.25, 6.75), height_ratios=[6, .75], width_ratios=[.75, 11, .5])
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=.0, hspace=.0)

plt.sca(ax[0, 1])
plt.gca().grid(False)
im = plt.gca().imshow(np.flipud(me_y_2d.T),
               alpha=0.5, cmap='cool', aspect='auto')

plt.gca().tick_params(
    axis='x', which='both', top=False, bottom=False, labelbottom=False)
plt.gca().tick_params(
    axis='y', which='both', left=False, right=False, labelleft=False)

plt.title('Two-dimensional PD for the Iris data set')

plt.colorbar(
    im, cax=ax[0, 2],
    label=f'{target_names[class_id]} probability', orientation='vertical')

# Rug plot X
plt.sca(ax[1, 1])

kde1 = scipy.stats.gaussian_kde(X_num[:, features_id[0]])
rug_x = np.linspace(
    X_num[:, features_id[0]].min(),
    X_num[:, features_id[0]].max(),
    num=200)
plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

plt.scatter(
    X_num[:, features_id[0]], [0]*len(X_num[:, features_id[0]]),
    c='k', marker='|', s=100, alpha=.33, zorder=10)

#plt.gca().set_xticks(
#    np.arange(ice_pd_2d['values'][0].shape[0], step=3),
#    labels=[f'{v:1.1f}' for i, v in enumerate(ice_pd_2d['values'][0]) if not i%3])
plt.xlim([X_num[:, features_id[0]].min(), X_num[:, features_id[0]].max()])
plt.xlabel(f'{feature_names[features_id[0]]} value')

plt.gca().tick_params(
    axis='y', which='both', left=False, right=False, labelleft=False)
plt.gca().invert_yaxis()

# Rug plot Y
plt.sca(ax[0, 0])

kde2 = scipy.stats.gaussian_kde(X_num[:, features_id[1]])
rug_y = np.linspace(
    X_num[:, features_id[1]].min(),
    X_num[:, features_id[1]].max(),
    num=200)
plt.plot(kde2(rug_y), rug_y, '-', alpha=.25)
plt.fill_betweenx(rug_y, kde2(rug_y), alpha=.25, zorder=5)

plt.scatter(
    [0]*len(X_num[:, features_id[1]]), X_num[:, features_id[1]],
    c='k', marker='_', s=100, alpha=.33, zorder=10)

#plt.gca().set_yticks(
#    np.arange(ice_pd_2d['values'][1].shape[0], step=3),
#    labels=[f'{v:1.1f}' for i, v in enumerate(ice_pd_2d['values'][1]) if not i%3][::-1])
plt.ylim([X_num[:, features_id[1]].min(), X_num[:, features_id[1]].max()])
plt.ylabel(f'{feature_names[features_id[1]]} value')

plt.gca().tick_params(
    axis='x', which='both', top=False, bottom=False, labelbottom=False)
plt.gca().invert_xaxis()

fig.delaxes(ax[1, 0])
fig.delaxes(ax[1, 2])
fig.tight_layout()

plt.show()
```

## Strict ME for Crisp Classifiers

```{python}
#| echo: false
#| output: false

class_id = 0
feature_id = 2

clf_num_crisp = sklearn.svm.SVC(probability=False)
clf_num_crisp.fit(X_num, y_num)

me_x, me_y, me_y_std = me_strict(
    X_num, clf_num_crisp.predict, feature_id, class_id, crisp=True)
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Crisp classification strict ME for the Iris data set
#| fig-width: 55%

fig, ax = plt.subplots(2, 1, figsize=(8, 4.5), height_ratios=[4, .5], sharex=True)
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=0, hspace=.05)

# PD
plt.sca(ax[0])

plt.title(f'Strict ME for the Iris data set')

for i in range(me_y.shape[1]):
    plt.plot(me_x,
             me_y[:, i],
             label=f'{target_names[i]}',
             alpha=.5)
plt.legend(loc='upper right', frameon=True, fancybox=True,
           labelcolor='black', framealpha=.5)

plt.ylabel('prediction count')

# rug plot
plt.sca(ax[1])

plt.xlabel(f'{feature_names[feature_id]} value')
plt.yticks([])

kde1 = scipy.stats.gaussian_kde(X_num[:, feature_id])
rug_x = np.linspace(
    X_num[:, feature_id].min(),
    X_num[:, feature_id].max(),
    num=200)
plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

plt.scatter(
    X_num[:, feature_id], [0]*len(X_num[:, feature_id]),
    c='k', marker='|', s=100, alpha=.33, zorder=10)

plt.show()
```

## Relaxed ME

```{python}
#| echo: false
#| output: false

# Generate M
class_id = 0
feature_id = 2
me_edges, me_x, me_y, me_y_std, me_x_counts = me_relaxed(
    X_num, clf_num.predict_proba, feature_id, class_id)
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Relaxed ME
#| fig-width: 55%

fig, ax = plt.subplots(2, 1, figsize=(8, 4.5), height_ratios=[4, .5], sharex=True)
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=0, hspace=.05)

# ME
plt.sca(ax[0])

plt.title(f'Relaxed ME for the Iris data set')

plt.plot(me_x, me_y, zorder=10)
mask = np.isfinite(me_y)
plt.plot(me_x[mask], me_y[mask], ls='--', c=line_colour, zorder=5)
plt.vlines(me_x, 0, 1, alpha=1.0, zorder=0,
            lw=1.0, ls=(0, (3, 5, 1, 5)), color=other_colour)

plt.ylabel(f'{target_names[class_id]} probability')
plt.gca().yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
# plt.gca().set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

# rug plot
plt.sca(ax[1])

plt.xlabel(f'{feature_names[feature_id]} value')
#plt.yticks([])

plt.bar(me_x, me_x_counts, 0.25, alpha=.75)
#plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
#plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

plt.show()
```

## Relaxed ME with Standard Deviation

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Relaxed ME with Standard Deviation
#| fig-width: 55%

fig, ax = plt.subplots(2, 1, figsize=(8, 4.5), height_ratios=[4, .5], sharex=True)
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=0, hspace=.05)

# ME
plt.sca(ax[0])

plt.title(f'Relaxed ME for the Iris data set')

plt.plot(me_x, me_y, zorder=10)
plt.fill_between(
    me_x,
    (me_y - me_y_std),
    (me_y + me_y_std),
    color=line_colour,
    alpha=.33,
    zorder=7)
mask = np.isfinite(me_y)
plt.plot(me_x[mask], me_y[mask], ls='--', c=line_colour, zorder=5)
plt.vlines(me_x, 0, 1, alpha=1.0, zorder=0,
            lw=1.0, ls=(0, (3, 5, 1, 5)), color=other_colour)

plt.ylabel(f'{target_names[class_id]} probability')
plt.gca().yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
# plt.gca().set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

# rug plot
plt.sca(ax[1])

plt.xlabel(f'{feature_names[feature_id]} value')
#plt.yticks([])

plt.bar(me_x, me_x_counts, 0.25, alpha=.75)
#plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
#plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

plt.show()
```

<!-- #%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#% --->

# Case Studies & Gotchas!

```{python}
#| echo: false
#| output: false

clf_num_linear = sklearn.svm.SVC(probability=True, kernel='linear')
clf_num_linear.fit(X_num, y_num)
```

## Feature Correlation

```{python}
#| echo: false
#| output: false

# Generate M
class_id = 0
features_id = [0, 2]
me_tuples = [
  me_relaxed(X_num, clf_num_linear.predict_proba, i, class_id)
  for i in features_id
]
```

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Relaxed ME of a single class for two correlated features
#| fig-width: 55%

fig, ax = plt.subplots(
    2, 2, figsize=(16, 4.5), height_ratios=[4, .5])
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=.05, hspace=.1)
fig.suptitle('Relaxed ME for all instances of the Iris data set for the '
             f'(probability of) {target_names[class_id]} class' )
fig.subplots_adjust(top=.92)

for tuple_id, me_tuple in enumerate(me_tuples):
    me_edges, me_x, me_y, me_y_std, me_x_counts = me_tuple

    # ME
    plt.sca(ax[0][tuple_id])

    plt.plot(me_x, me_y, zorder=10, c=line_colour)
    plt.fill_between(
        me_x,
        (me_y - me_y_std),
        (me_y + me_y_std),
        color=line_colour,
        alpha=.33,
        zorder=7)
    mask = np.isfinite(me_y)
    plt.plot(me_x[mask], me_y[mask], ls='--', c=line_colour, zorder=5)
    plt.vlines(me_x, 0, 1, alpha=1.0, zorder=0,
                lw=1.0, ls=(0, (3, 5, 1, 5)), color=other_colour)

    #plt.ylabel(f'{target_names[class_id]} probability')
    plt.gca().yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
    # plt.gca().set_yticklabels([f'{100*item:2.0f}%' for item in ax.get_yticks()])

    # rug plot
    plt.sca(ax[1][tuple_id])

    plt.xlabel(f'{feature_names[features_id[tuple_id]]} value')
    #plt.yticks([])

    plt.bar(me_x, me_x_counts, 0.5*(me_x.max()-me_x.min())/10, alpha=.75)
    #plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
    #plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

ax[0][0].tick_params(
    axis='x', which='both', top=False, bottom=False, labelbottom=False)
ax[0][1].tick_params(
    axis='x', which='both', top=False, bottom=False, labelbottom=False)

ax[0][1].tick_params(
    axis='y', which='both', left=False, right=False, labelleft=False)
ax[1][1].tick_params(
    axis='y', which='both', left=False, right=False, labelleft=False)

ax[0][0].sharex(ax[1][0])
ax[0][1].sharex(ax[1][1])

ax[0][0].sharey(ax[0][1])
ax[1][0].sharey(ax[1][1])

plt.show()
```

## Feature Correlation {{< meta subs.ctd >}}

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: PD of a single class for two correlated features
#| fig-width: 55%

fig, ax = plt.subplots(
    2, 2, figsize=(16, 4.5), height_ratios=[4, .5])
fig.patch.set_alpha(0)
plt.subplots_adjust(wspace=.05, hspace=.1)
fig.suptitle('PD for all instances of the Iris data set for the '
             f'(probability of) {target_names[class_id]} class')
fig.subplots_adjust(top=.92)

for feature_id_cut, feature_id_full in enumerate(features_id):

    ice_pd_num_s = sklearn.inspection.partial_dependence(
        clf_num_linear, X_num, features=[feature_id_full],
        percentiles=(0, 1), grid_resolution=500,
        response_method='predict_proba', kind='both')
    vce_pd_num_s_std = np.std(
        ice_pd_num_s['individual'][class_id], axis=0)

    # PD
    plt.sca(ax[0][feature_id_cut])

    plt.plot(
        ice_pd_num_s['values'][0],
        ice_pd_num_s['average'][class_id],
        lw=2, zorder=10, c=line_colour)
    plt.plot(
        ice_pd_num_s['values'][0],
        ice_pd_num_s['individual'][class_id].T,
        alpha=.05, c=other_colour, zorder=0)
    plt.fill_between(
        ice_pd_num_s['values'][0],
        (ice_pd_num_s['average'][class_id] - vce_pd_num_s_std),
        (ice_pd_num_s['average'][class_id] + vce_pd_num_s_std),
        color=line_colour,
        alpha=.25, zorder=5)

    plt.gca().yaxis.set_major_formatter(lambda f, pos: f'{100*f:2.0f}%')
    plt.ylim((-0.1723, 1.0437))

    # rug plot
    plt.sca(ax[1][feature_id_cut])

    plt.xlabel(f'{feature_names[feature_id_full]} value')
    plt.yticks([])

    kde1 = scipy.stats.gaussian_kde(X_num[:, feature_id_full])
    rug_x = np.linspace(
        X_num[:, feature_id_full].min(),
        X_num[:, feature_id_full].max(),
        num=200)
    plt.plot(rug_x, kde1(rug_x), '-', alpha=.25)
    plt.fill_between(rug_x, kde1(rug_x), alpha=.25, zorder=5)

    plt.scatter(
        X_num[:, feature_id_full], [0]*len(X_num[:, feature_id_full]),
        c='k', marker='|', s=100, alpha=.33, zorder=10)

ax[0][0].tick_params(
    axis='x', which='both', top=False, bottom=False, labelbottom=False)
ax[0][1].tick_params(
    axis='x', which='both', top=False, bottom=False, labelbottom=False)

ax[0][1].tick_params(
    axis='y', which='both', left=False, right=False, labelleft=False)

ax[0][0].sharex(ax[1][0])
ax[0][1].sharex(ax[1][1])

ax[0][0].sharey(ax[0][1])

plt.show()
```

## Feature Correlation {{< meta subs.ctd >}}

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Model coefficients for the selected class
#| fig-width: 55%

fig, ax = plt.subplots(figsize=(6, 4))
fig.patch.set_alpha(0)

r = list(range(clf_num_linear.coef_.shape[1]))[::-1]
plt.barh(r, clf_num_linear.coef_[class_id, :])
plt.gca().set_yticks(r, labels=feature_names)
plt.title(f'Coefficients of a linear SVM for the {target_names[class_id]} class')

plt.show()
```

## Feature Correlation {{< meta subs.ctd >}}

```{python}
#| echo: false
#| fig-align: center
#| fig-alt: Iris feature correlation
#| fig-width: 55%

X_num_corr = np.flipud(np.corrcoef(X_num, rowvar=False))

plt.style.use('default')
update_colours()

fig, ax = plt.subplots()
fig.patch.set_alpha(0)

im = ax.imshow(np.abs(X_num_corr), vmin=0, vmax=1, alpha=.5, cmap='bwr')

# Show all ticks and label them with the respective list entries
ax.set_xticks(np.arange(len(feature_names)), labels=feature_names)
ax.set_yticks(np.arange(len(feature_names)), labels=feature_names[::-1])

# Rotate the tick labels and set their alignment.
plt.setp(ax.get_xticklabels(), rotation=15, ha='right',
         rotation_mode='anchor')
plt.setp(ax.get_yticklabels(), rotation=0, ha='right',
         rotation_mode='anchor')

# Loop over data dimensions and create text annotations.
for i in range(len(feature_names)):
    for j in range(len(feature_names)):
        text = ax.text(j, i, f'{X_num_corr[i, j]:1.2f}',
                       ha='center', va='center', color='k')

ax.set_title('Correlation coefficient between Iris features')
fig.tight_layout()
plt.show()

plt.style.use('seaborn')
update_colours()
```

::: {.notes}
- ME reports feature influence, but beacause of averaging only similar points
  the measurement gets distorted by feature correlation, therefore reporting
  the **combined effect**
- Remember that we are working with **conditional distributions**
- In this example, feature #1 (*sepal length*) -- whose coefficient is close
  to 0 (-0.06) -- shows strong influence, whcih is due to it being heavily
  correlated (0.87) with feature #3 (*petal length*) -- whose coefficient has
  the largest magnitude (close to -1.0)
- PD, which is largely immune to this effect, does not display this behaviour
:::

<!-- #%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#% --->

# Properties

## Pros &nbsp;&nbsp;&nbsp;{{< fa plus-square >}}

- **Easy to generate and interpret**
- Based on **real data**

## Cons &nbsp;&nbsp;&nbsp;{{< fa minus-square >}}

- Assumes **feature independence**, which is often unreasonable and
  heavily biases the influence measurements
- May be **unreliable for certain values** of the explained feature when
  there is a low number of data points with that value (**strict**) or
  in a relevant bin (**relaxed**);
  this impacts the reliability of influence estimates
  (average perdiction of the explained model for that value or range of values)
- Reliability of estimates can only be communicated by displaying a rug plot
  or distribution of instances per value or bin
- Diversity (heterogeneity) of the model's behaviour for each particular
  value or bin can only be communicated by **prediction variance**
- Limited to explaining **two feature at a time**

## Caveats &nbsp;&nbsp;&nbsp;{{< fa skull >}}

- The measurements may be sensitive to different binning approaches for
  **relaxed** ME
- Computational complexity: $\mathcal{O} \left( n \right)$, where
  $n$ is the number of instances in the designated data set

<!-- #%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#%#% --->

# Further Considerations

## Related Techniques

<br>

### Accumulated Local Effect (ALE)

> [{{< fa person-chalkboard >}}](ale.html) &nbsp;&nbsp;&nbsp;
> An evolved version of (relaxed) ME that is less prone to being affected
> by feature correlation.
> It communicates the influence of a specific feature value on the model's
> prediction by quantifying the average (accumulated) difference between
> the predictions at the boundaries of a (small) **fixed interval** around
> the selected feature value [@apley2020visualizing].
> It is calculated by replacing the value of the explained feature with the
> interval boundaries for **instances found in the designated data set**
> whose value of this feature is within the specified range.

## Related Techniques {{< meta subs.ctd >}}

<br>

### Individual Conditional Expectation (ICE)

> [{{< fa person-chalkboard >}}](ice.html) &nbsp;&nbsp;&nbsp;
> It communicates the influence of a specific feature value on
> the model's prediction by **fixing the value of this feature**
> across a designated range for a selected data point [@goldstein2015peeking].
> It is an instance-focused (local) "variant" of *Partial Dependence*.

## Related Techniques {{< meta subs.ctd >}}

<br>

### Partial Dependence (PD)

> [{{< fa person-chalkboard >}}](pd.html) &nbsp;&nbsp;&nbsp;
> It communicates the average influence of a specific feature value on
> the model's prediction by **fixing the value of this feature** across a
> designated range for a set of instances.
> It is a model-focused (global) "variant" of
> *Individual Conditional Expectation*, which is calculated by **averaging**
> ICE across a collection of data points [@friedman2001greedy].

## Implementations

| {{< fa brands python >}} Python          | {{< fa brands r-project >}} R     |
|:-----------------------------------------|:----------------------------------|
| N/A                                      | [DALEX]                           |

: {tbl-colwidths="[50,50]"}

## Further Reading

- [*Interpretable Machine Learning* book][iml-book]
- [*Explanatory Model Analysis* book][ema-book]

## Bibliography

::: {#refs}
:::

---

[DALEX]: https://github.com/ModelOriented/DALEX

[iml-book]: https://christophm.github.io/interpretable-ml-book/ale.html
[ema-book]: https://ema.drwhy.ai/accumulatedLocalProfiles.html#local-dependence-profile
